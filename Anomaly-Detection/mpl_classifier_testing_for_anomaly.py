# -*- coding: utf-8 -*-
"""MPL classifier Testing for Anomaly.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MfuU5Kw65S2yAkuIKsh1zp-lfqOwDo7P
"""

import numpy as np
import matplotlib.pyplot as plt
import math

# Parameters
m = 2  # The multiplier in the equation y = m * e^x
num_samples = 100  # Number of samples to generate
x_min = 0  # Minimum value of x
x_max = 10  # Maximum value of x

# Generate random x values within the range [x_min, x_max]
x_values = np.random.uniform(x_min, x_max, num_samples)

# Compute corresponding y values using the equation y = m * e^x
y_values = m * np.exp(x_values)

# Plot the data
plt.figure(figsize=(10, 6))
plt.scatter(x_values, y_values, color='blue', label='Generated data points')
plt.title('Generated Data for y = m * e^x')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.show()

import numpy as np
num_samples = 200  # Number of samples to generate
num_anomalies = 20
anomaly_indices = np.random.choice(range(num_samples//2, num_samples//2 + num_anomalies), num_anomalies, replace=False)

print(anomaly_indices)


x= np.random.uniform(0.1, 0.3, num_anomalies)

print(x)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix

# Parameters
m = 2  # The multiplier in the equation y = m * e^x
num_samples = 200  # Number of samples to generate
num_anomalies = 20  # Number of anomalies to inject
x_min = 0  # Minimum value of x
x_max = 10  # Maximum value of x

# Generate random x values within the range [x_min, x_max]
np.random.seed(42)  # For reproducibility
x_values = np.random.uniform(x_min, x_max, num_samples)
y_values = m * np.exp(x_values)

# Inject anomalies in the middle of the data points
anomaly_indices = np.random.choice(range(num_samples//2, num_samples//2 + num_anomalies), num_anomalies, replace=False)
y_values[anomaly_indices] *= np.random.uniform(0.01, 0.09, num_anomalies)  # Inject anomalies by scaling down

# Create labels for the data: 0 for normal data, 1 for anomalies
labels = np.zeros(num_samples)
labels[anomaly_indices] = 1

# Plot the data
plt.figure(figsize=(10, 6))
plt.scatter(x_values, y_values, c=labels, cmap='coolwarm', label='Data points')
plt.title('Generated Data for y = m * e^x with Anomalies')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.show()

# Prepare the data for training
# Features: x_values and y_values
# Labels: 0 for normal data, 1 for anomalies
X = np.column_stack((x_values, y_values))
y = labels

# Standardize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train an MPL classifier
mlp = MLPClassifier(hidden_layer_sizes=(500, 300), max_iter=1000, random_state=42)
mlp.fit(X_scaled, y)

# Predict and evaluate the model
y_pred = mlp.predict(X_scaled)
print(classification_report(y, y_pred))
print(confusion_matrix(y, y_pred))

print(f"predicted values are : {y}")
# Plot the predictions
plt.figure(figsize=(10, 6))
plt.scatter(x_values, y_values, c=y_pred, cmap='coolwarm', label='Predicted anomalies')
plt.title('MPL Classifier Prdictions')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.show()

"""#AutoEncoder NN for detected anomalies"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense

# Parameters
m = 2  # The multiplier in the equation y = m * e^x
num_samples = 200  # Number of samples to generate
num_anomalies = 20  # Number of anomalies to inject
x_min = 0  # Minimum value of x
x_max = 10  # Maximum value of x

# Generate random x values within the range [x_min, x_max]
np.random.seed(42)  # For reproducibility
x_values = np.random.uniform(x_min, x_max, num_samples)
y_values = m * np.exp(x_values)

# Inject anomalies in the middle of the data points
anomaly_indices = np.random.choice(range(num_samples//2, num_samples//2 + num_anomalies), num_anomalies, replace=False)
y_values[anomaly_indices] *= np.random.uniform(0.01, 0.9, num_anomalies)  # Inject anomalies by scaling down

# Create labels for the data: 0 for normal data, 1 for anomalies
labels = np.zeros(num_samples)
labels[anomaly_indices] = 1

# Prepare the data for training
# Features: x_values and y_values
# Labels: 0 for normal data, 1 for anomalies
X = np.column_stack((x_values, y_values))
y = labels

# Standardize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Build the autoencoder
input_dim = X_scaled.shape[1]
encoding_dim = 2

input_layer = Input(shape=(input_dim,))
encoder = Dense(encoding_dim, activation='relu')(input_layer)
decoder = Dense(input_dim, activation='sigmoid')(encoder)
autoencoder = Model(inputs=input_layer, outputs=decoder)

autoencoder.compile(optimizer='adam', loss='mean_squared_error')

# Train the autoencoder
autoencoder.fit(X_scaled, y, epochs=100, batch_size=10, shuffle=True, validation_split=0.2, verbose=1)

# Predict the reconstruction of the data
X_pred = autoencoder.predict(X_scaled)

# Calculate the reconstruction error
reconstruction_error = np.mean(np.power(X_scaled - X_pred, 2), axis=1)

# Set a threshold for reconstruction error
threshold = np.percentile(reconstruction_error, 95)
y_pred = (reconstruction_error > threshold).astype(int)

# Evaluate the model
print(classification_report(y, y_pred))
print(confusion_matrix(y, y_pred))

# Plot the data and predictions in two subplots
fig, ax = plt.subplots(1, 2, figsize=(14, 6))

# Original data with anomalies
ax[0].scatter(x_values, y_values, c=labels, cmap='coolwarm', label='Data points')
ax[0].set_title('Generated Data for y = m * e^x with Anomalies')
ax[0].set_xlabel('x')
ax[0].set_ylabel('y')
ax[0].legend()

# Autoencoder predictions
ax[1].scatter(x_values, y_values, c=y_pred, cmap='coolwarm', label='Predicted anomalies')
ax[1].set_title('Autoencoder Predictions')
ax[1].set_xlabel('x')
ax[1].set_ylabel('y')
ax[1].legend()

plt.tight_layout()
plt.show()

"""## LSTM approach ( for non temporal data )"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import Input, LSTM, Dense, RepeatVector, TimeDistributed

# Parameters
m = 2  # The multiplier in the equation y = m * e^x
num_samples = 200  # Number of samples to generate
num_anomalies = 20  # Number of anomalies to inject
x_min = 0  # Minimum value of x
x_max = 10  # Maximum value of x

# Generate random x values within the range [x_min, x_max]
np.random.seed(42)  # For reproducibility
x_values = np.random.uniform(x_min, x_max, num_samples)
y_values = m * np.exp(x_values)

# Inject anomalies in the middle of the data points
anomaly_indices = np.random.choice(range(num_samples//2, num_samples//2 + num_anomalies), num_anomalies, replace=False)
y_values[anomaly_indices] *= np.random.uniform(0.01, 0.9, num_anomalies)  # Inject anomalies by scaling down

# Create labels for the data: 0 for normal data, 1 for anomalies
labels = np.zeros(num_samples)
labels[anomaly_indices] = 1

# Prepare the data for training
# Features: x_values and y_values
# Labels: 0 for normal data, 1 for anomalies
X = np.column_stack((x_values, y_values))
y = labels

# Standardize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Reshape data to 3D for LSTM (samples, timesteps, features)
X_scaled = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))

# Build the LSTM autoencoder
model = Sequential()
model.add(LSTM(64, activation='relu', input_shape=(X_scaled.shape[1], X_scaled.shape[2]), return_sequences=True))
model.add(LSTM(32, activation='relu', return_sequences=False))
model.add(RepeatVector(X_scaled.shape[1]))
model.add(LSTM(32, activation='relu', return_sequences=True))
model.add(LSTM(64, activation='relu', return_sequences=True))
model.add(TimeDistributed(Dense(X_scaled.shape[2])))

model.compile(optimizer='adam', loss='mse')
model.summary()

# Train the LSTM autoencoder
model.fit(X_scaled, X_scaled, epochs=100, batch_size=10, shuffle=True, validation_split=0.2, verbose=1)

# Predict the reconstruction of the data
X_pred = model.predict(X_scaled)

# Calculate the reconstruction error
reconstruction_error = np.mean(np.power(X_scaled - X_pred, 2), axis=(1, 2))

# Analyze the error distribution to set the threshold
plt.figure(figsize=(10, 6))
plt.hist(reconstruction_error, bins=50, color='blue', alpha=0.7)
plt.title('Reconstruction Error Distribution')
plt.xlabel('Reconstruction error')
plt.ylabel('Frequency')
plt.show()

# Set a threshold for reconstruction error
threshold = np.percentile(reconstruction_error, 95)  # 95th percentile
y_pred = (reconstruction_error > threshold).astype(int)

# Evaluate the model
print(classification_report(y, y_pred))
print(confusion_matrix(y, y_pred))

# Plot the data and predictions in two subplots
fig, ax = plt.subplots(1, 2, figsize=(14, 6))

# Original data with anomalies
ax[0].scatter(x_values, y_values, c=labels, cmap='coolwarm', label='Data points')
ax[0].set_title('Generated Data for y = m * e^x with Anomalies')
ax[0].set_xlabel('x')
ax[0].set_ylabel('y')
ax[0].legend()

# LSTM autoencoder predictions
ax[1].scatter(x_values, y_values, c=y_pred, cmap='coolwarm', label='Predicted anomalies')
ax[1].set_title('LSTM Autoencoder Predictions')
ax[1].set_xlabel('x')
ax[1].set_ylabel('y')
ax[1].legend()

plt.tight_layout()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Parameters
num_unique_values = 20  # Number of unique values for the new column
num_x_per_value = 9  # Number of different x values per unique value
m_min, m_max = 1, 3  # Range for random m values
a_min, a_max = 0.5, 1.5  # Range for random a values
x_min, x_max = 0, 10  # Range for x values
num_anomalies = 20  # Number of anomalies to inject

# Generate the dataset
np.random.seed(42)  # For reproducibility
unique_values = np.arange(num_unique_values)  # Unique values for the new column
x_values = np.random.uniform(x_min, x_max, num_unique_values * num_x_per_value)  # Random x values
m_values = np.random.uniform(m_min, m_max, num_unique_values)  # Random m values for each unique value
a_values = np.random.uniform(a_min, a_max, num_unique_values)  # Random a values for each unique value

# Calculate y values
y_values = np.zeros(num_unique_values * num_x_per_value)
for i in range(num_unique_values):
    start_idx = i * num_x_per_value
    end_idx = start_idx + num_x_per_value
    y_values[start_idx:end_idx] = m_values[i] * np.exp(a_values[i] * x_values[start_idx:end_idx])

# Create labels for the data: 0 for normal data, 1 for anomalies
labels = np.zeros(num_unique_values * num_x_per_value)

# Inject anomalies in the middle of the data points
anomaly_indices = np.random.choice(range(num_unique_values * num_x_per_value // 2,
                                         num_unique_values * num_x_per_value // 2 + num_anomalies),
                                   num_anomalies, replace=False)
y_values[anomaly_indices] *= np.random.uniform(0.01, 0.9, num_anomalies)  # Inject anomalies by scaling down
labels[anomaly_indices] = 1

# Prepare the data for training
# Features: x_values, a_values, and y_values
# Labels: 0 for normal data, 1 for anomalies
new_column = np.repeat(unique_values, num_x_per_value)
X = np.column_stack((new_column, x_values, y_values))
y = labels


print(X)

"""## Re-Testing MLP Classifier for anomaly detection"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, confusion_matrix
from mpl_toolkits.mplot3d import Axes3D
# Parameters
num_unique_values = 20  # Number of unique values for the new column
num_x_per_value = 8  # Number of different x values per unique value (fixed x values given)
m_min, m_max = 10, 15  # Range for random m values
a_min, a_max = 5, 10  # Range for random a values (increased to create more curving nature)

# Fixed x values
x_values = np.array([0.65, 0.70, 0.75, 0.80, 0.85, 0.95, 1.0, 1.05])

# Generate the dataset
np.random.seed(42)
unique_values = np.arange(num_unique_values)  # Unique values for the new column
m_values = np.random.uniform(m_min, m_max, num_unique_values)  # Random m values for each unique value
a_values = np.random.uniform(a_min, a_max, num_unique_values)  # Random a values for each unique value

# Calculate y values (ensuring a decreasing curve)
y_values = np.zeros(num_unique_values * num_x_per_value)
for i in range(num_unique_values):
    start_idx = i * num_x_per_value
    end_idx = start_idx + num_x_per_value
    y_values[start_idx:end_idx] = m_values[i] * np.exp(-a_values[i] * x_values)

# Create labels for the data: 0 for normal data, 1 for anomalies
labels = np.zeros(num_unique_values * num_x_per_value)

# Inject one anomaly in each group
for i in range(num_unique_values):
    start_idx = i * num_x_per_value
    end_idx = start_idx + num_x_per_value
    anomaly_index = np.random.choice(range(start_idx, end_idx))
    y_values[anomaly_index] *= np.random.uniform(0.01, 0.05)  # Inject anomalies by scaling down
    labels[anomaly_index] = 1

# Prepare the data for training
# Features: new_column, x_values, and y_values
# Labels: 0 for normal data, 1 for anomalies
new_column = np.repeat(unique_values, num_x_per_value)
X = np.column_stack((new_column, np.tile(x_values, num_unique_values), y_values))
y = labels

# Standardize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the data into training and testing sets (if needed)
# For simplicity, we'll use the entire dataset for training and testing in this example
X_train, y_train = X_scaled, y

# Build the MLP classifier
mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)

# Train the MLP classifier
mlp.fit(X_train, y_train)

# Predict using the trained model
y_pred = mlp.predict(X_train)

# Evaluate the model
print(classification_report(y_train, y_pred))
print(confusion_matrix(y_train, y_pred))

# Plot the data and predictions for each group level
fig, axes = plt.subplots(num_unique_values, 2, figsize=(10, num_unique_values * 3))

for i in range(num_unique_values):
    start_idx = i * num_x_per_value
    end_idx = start_idx + num_x_per_value
    x_group = x_values
    y_group = y_values[start_idx:end_idx]
    labels_group = labels[start_idx:end_idx]
    y_pred_group = y_pred[start_idx:end_idx]

    # Original data with anomalies for each group
    scatter1 = axes[i, 0].scatter(x_group, y_group, c=labels_group, cmap='coolwarm')
    axes[i, 0].set_title(f'Group {i}: Original Data with Anomalies')
    axes[i, 0].set_xlabel('x')
    axes[i, 0].set_ylabel('y')
    if i == 0:
        legend1 = axes[i, 0].legend(*scatter1.legend_elements(), title="Labels")
        axes[i, 0].add_artist(legend1)

    # MLP classifier predictions for each group
    scatter2 = axes[i, 1].scatter(x_group, y_group, c=y_pred_group, cmap='coolwarm')
    axes[i, 1].set_title(f'Group {i}: MLP Classifier Predictions')
    axes[i, 1].set_xlabel('x')
    axes[i, 1].set_ylabel('y')
    if i == 0:
        legend2 = axes[i, 1].legend(*scatter2.legend_elements(), title="Predictions")
        axes[i, 1].add_artist(legend2)

plt.tight_layout()
plt.show()

# 3D plot of the entire dataset
fig = plt.figure(figsize=(15, 10))
ax = fig.add_subplot(111, projection='3d')
correct_anomalies = (y == 1) & (y_pred == 1)
scatter = ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=labels, cmap='coolwarm', label='Original Labels',zorder=2)
scatter_correct = ax.scatter(X[correct_anomalies, 0], X[correct_anomalies, 1], X[correct_anomalies, 2],
                             c='green', marker='^', label='Correct Anomalies',zorder=1)

ax.set_xlabel('Group')
ax.set_ylabel('X')
ax.set_zlabel('Y')
ax.set_title('3D Scatter Plot of the Entire Dataset')

legend = ax.legend()
ax.add_artist(legend)
plt.show()

"""## Using MLP , first we traing on 1000x8 rows data and the use 20x8 ddata for prediction"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, confusion_matrix
from mpl_toolkits.mplot3d import Axes3D
from sklearn.decomposition import PCA

# Parameters
num_unique_train_values = 1000  # Number of unique values for the new column in the training dataset
num_unique_test_values = 20  # Number of unique values for the new column in the testing dataset
num_x_per_value = 9  # Number of different x values per unique value (fixed x values given)
m_min, m_max = 10, 15  # Range for random m values
a_min, a_max = 5, 10  # Range for random a values (increased to create more curving nature)

# Fixed x values
x_values = np.array([0.65, 0.70, 0.75, 0.80, 0.85,0.90,0.95, 1.0, 1.05])

# Generate the training dataset
np.random.seed(42)  # For reproducibility
train_unique_values = np.arange(num_unique_train_values)  # Unique values for the new column in the training dataset
train_m_values = np.random.uniform(m_min, m_max, num_unique_train_values)  # Random m values for each unique value
train_a_values = np.random.uniform(a_min, a_max, num_unique_train_values)  # Random a values for each unique value

# Calculate y values (ensuring a decreasing curve) for training data
train_y_values = np.zeros(num_unique_train_values * num_x_per_value)
for i in range(num_unique_train_values):
    start_idx = i * num_x_per_value
    end_idx = start_idx + num_x_per_value
    train_y_values[start_idx:end_idx] = train_m_values[i] * np.exp(-train_a_values[i] * x_values)

# Prepare the training data
train_new_column = np.repeat(train_unique_values, num_x_per_value)
X_train = np.column_stack((train_new_column, np.tile(x_values, num_unique_train_values), train_y_values))

# Standardize the training data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

pca = PCA(n_components=0.95)  # Retain 95% of variance
X_train_scaled = pca.fit_transform(X_train_scaled)


# Generate the testing dataset
test_unique_values = np.arange(num_unique_test_values)  # Unique values for the new column in the testing dataset
test_m_values = np.random.uniform(m_min, m_max, num_unique_test_values)  # Random m values for each unique value
test_a_values = np.random.uniform(a_min, a_max, num_unique_test_values)  # Random a values for each unique value

# Calculate y values (ensuring a decreasing curve) for testing data
test_y_values = np.zeros(num_unique_test_values * num_x_per_value)
for i in range(num_unique_test_values):
    start_idx = i * num_x_per_value
    end_idx = start_idx + num_x_per_value
    test_y_values[start_idx:end_idx] = test_m_values[i] * np.exp(-test_a_values[i] * x_values)

# Create labels for the test data: 0 for normal data, 1 for anomalies
test_labels = np.zeros(num_unique_test_values * num_x_per_value)

# Inject one anomaly in each group for testing data
for i in range(num_unique_test_values):
    start_idx = i * num_x_per_value
    end_idx = start_idx + num_x_per_value
    anomaly_index = np.random.choice(range(start_idx, end_idx))
    test_y_values[anomaly_index] *= np.random.uniform(0.01, 0.9)  # Inject anomalies by scaling down
    test_labels[anomaly_index] = 1

# Prepare the testing data
test_new_column = np.repeat(test_unique_values, num_x_per_value)
X_test = np.column_stack((test_new_column, np.tile(x_values, num_unique_test_values), test_y_values))
y_test = test_labels

# Standardize the testing data
X_test_scaled = scaler.transform(X_test)
X_test_scaled = pca.transform(X_test_scaled)
# Build the MLP classifier
mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)

# Train the MLP classifier
mlp.fit(X_train_scaled, np.zeros(X_train_scaled.shape[0]))  # Training with no anomalies

# Predict using the trained model
y_pred = mlp.predict(X_test_scaled)

# Evaluate the model
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

# Plot the data and predictions for each group level
fig, axes = plt.subplots(num_unique_test_values, 2, figsize=(10, num_unique_test_values * 3))

for i in range(num_unique_test_values):
    start_idx = i * num_x_per_value
    end_idx = start_idx + num_x_per_value
    x_group = x_values
    y_group = test_y_values[start_idx:end_idx]
    labels_group = test_labels[start_idx:end_idx]
    y_pred_group = y_pred[start_idx:end_idx]

    # Original data with anomalies for each group
    scatter1 = axes[i, 0].scatter(x_group, y_group, c=labels_group, cmap='coolwarm', zorder=1)
    # scatter_correct = axes[i, 0].scatter(x_group[labels_group == 1], y_group[labels_group == 1],
    #                                      c='green', marker='^', label='Correct Anomalies', zorder=2)
    axes[i, 0].set_title(f'Group {i}: Original Data with Anomalies')
    axes[i, 0].set_xlabel('x')
    axes[i, 0].set_ylabel('y')
    if i == 0:
        legend1 = axes[i, 0].legend(*scatter1.legend_elements(), title="Labels")
        axes[i, 0].add_artist(legend1)
        axes[i, 0].legend(loc='upper right')

    # MLP classifier predictions for each group
    scatter2 = axes[i, 1].scatter(x_group, y_group, c=y_pred_group, cmap='coolwarm', zorder=1)
    # scatter_correct_pred = axes[i, 1].scatter(x_group[y_pred_group == 1], y_group[y_pred_group == 1],
    #                                           c='green', marker='^', label='Correct Predictions', zorder=2)
    axes[i, 1].set_title(f'Group {i}: MLP Classifier Predictions')
    axes[i, 1].set_xlabel('x')
    axes[i, 1].set_ylabel('y')
    if i == 0:
        legend2 = axes[i, 1].legend(*scatter2.legend_elements(), title="Predictions")
        axes[i, 1].add_artist(legend2)
        axes[i, 1].legend(loc='upper right')

plt.tight_layout()
plt.show()

# 3D plot of the entire test dataset
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')
scatter = ax.scatter(X_test[:, 0], X_test[:, 1], X_test[:, 2], c=test_labels, cmap='coolwarm', label='Original Labels', zorder=1)
correct_anomalies = (y_test == 1) & (y_pred == 1)
scatter_correct = ax.scatter(X_test[correct_anomalies, 0], X_test[correct_anomalies, 1], X_test[correct_anomalies, 2],
                             c='green', marker='^', label='Correct Anomalies', zorder=2)

ax.set_xlabel('Group')
ax.set_ylabel('X')
ax.set_zlabel('Y')
ax.set_title('3D Scatter Plot of the Entire Test Dataset')

legend = ax.legend()
ax.add_artist(legend)

plt.show()

"""## New Test : Using PCA"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.decomposition import PCA

# Parameters
num_unique_train_values = 1000  # Number of unique values for the new column in the training dataset
num_unique_test_values = 20  # Number of unique values for the new column in the testing dataset
num_x_per_value = 9  # Number of different x values per unique value (fixed x values given)
m_min, m_max = 10, 15  # Range for random m values
a_min, a_max = 5, 10  # Range for random a values (ensuring a decreasing trend)

# Fixed x values
x_values = np.array([0.65, 0.70, 0.75, 0.80, 0.85,0.90, 0.95, 1.0, 1.05])

# Function to generate y values for each group ensuring a decreasing trend
def generate_y_values(m, a, x_values):
    return m * np.exp(-a * x_values)

# Generate the training dataset
np.random.seed(42)  # For reproducibility
train_unique_values = np.arange(num_unique_train_values)  # Unique values for the new column in the training dataset
train_m_values = np.random.uniform(m_min, m_max, num_unique_train_values)  # Random m values for each unique value
train_a_values = np.random.uniform(a_min, a_max, num_unique_train_values)  # Random a values for each unique value

# Calculate y values ensuring a decreasing trend for training data
train_y_values = np.concatenate([generate_y_values(train_m_values[i], train_a_values[i], x_values)
                                 for i in range(num_unique_train_values)])

# Prepare the training data
train_new_column = np.repeat(train_unique_values, num_x_per_value)
X_train = np.column_stack((train_new_column, np.tile(x_values, num_unique_train_values), train_y_values))

# Standardize the training data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

# Apply PCA to the training data
pca = PCA(n_components=0.95)  # Retain 95% of variance
X_train_pca = pca.fit_transform(X_train_scaled)

# Generate the testing dataset
test_unique_values = np.arange(num_unique_test_values)  # Unique values for the new column in the testing dataset
test_m_values = np.random.uniform(m_min, m_max, num_unique_test_values)  # Random m values for each unique value
test_a_values = np.random.uniform(a_min, a_max, num_unique_test_values)  # Random a values for each unique value

# Calculate y values ensuring a decreasing trend for testing data
test_y_values = np.concatenate([generate_y_values(test_m_values[i], test_a_values[i], x_values)
                                for i in range(num_unique_test_values)])

# Create labels for the test data: 0 for normal data, 1 for anomalies
test_labels = np.zeros(num_unique_test_values * num_x_per_value)

# Inject one anomaly in each group for testing data
for i in range(num_unique_test_values):
    start_idx = i * num_x_per_value
    end_idx = start_idx + num_x_per_value
    anomaly_index = np.random.choice(range(start_idx, end_idx))
    test_y_values[anomaly_index] *= np.random.uniform(0.01, 0.9)  # Inject anomalies by scaling down
    test_labels[anomaly_index] = 1

# Prepare the testing data
test_new_column = np.repeat(test_unique_values, num_x_per_value)
X_test = np.column_stack((test_new_column, np.tile(x_values, num_unique_test_values), test_y_values))
y_test = test_labels

# Standardize the testing data
X_test_scaled = scaler.transform(X_test)
X_test_pca = pca.transform(X_test_scaled)

# Build the MLP classifier
mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)

# Train the MLP classifier
mlp.fit(X_train_pca, np.zeros(X_train_pca.shape[0]))  # Training with no anomalies

# Predict using the trained model
y_pred = mlp.predict(X_test_pca)

# Evaluate the model
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

# Plot the data and predictions for each group level
fig, axes = plt.subplots(num_unique_test_values, 2, figsize=(10, num_unique_test_values * 3))

for i in range(num_unique_test_values):
    start_idx = i * num_x_per_value
    end_idx = start_idx + num_x_per_value
    x_group = x_values
    y_group = test_y_values[start_idx:end_idx]
    labels_group = test_labels[start_idx:end_idx]
    y_pred_group = y_pred[start_idx:end_idx]

    # Original data with anomalies for each group
    scatter1 = axes[i, 0].scatter(x_group, y_group, c=labels_group, cmap='coolwarm', zorder=1)
    axes[i, 0].set_title(f'Group {i}: Original Data with Anomalies')
    axes[i, 0].set_xlabel('x')
    axes[i, 0].set_ylabel('y')
    if i == 0:
        legend1 = axes[i, 0].legend(*scatter1.legend_elements(), title="Labels")
        axes[i, 0].add_artist(legend1)
        axes[i, 0].legend(loc='upper right')

    # MLP classifier predictions for each group
    scatter2 = axes[i, 1].scatter(x_group, y_group, c=y_pred_group, cmap='coolwarm', zorder=1)
    axes[i, 1].set_title(f'Group {i}: MLP Classifier Predictions')
    axes[i, 1].set_xlabel('x')
    axes[i, 1].set_ylabel('y')
    if i == 0:
        legend2 = axes[i, 1].legend(*scatter2.legend_elements(), title="Predictions")
        axes[i, 1].add_artist(legend2)
        axes[i, 1].legend(loc='upper right')

plt.tight_layout()
plt.show()

# 3D plot of the entire test dataset
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')
scatter = ax.scatter(X_test[:, 0], X_test[:, 1], X_test[:, 2], c=test_labels, cmap='coolwarm', label='Original Labels', zorder=1)
correct_anomalies = (y_test == 1) & (y_pred == 1)
scatter_correct = ax.scatter(X_test[correct_anomalies, 0], X_test[correct_anomalies, 1], X_test[correct_anomalies, 2],
                             c='green', marker='^', label='Correct Anomalies', zorder=2)

ax.set_xlabel('Group')
ax.set_ylabel('X')
ax.set_zlabel('Y')
ax.set_title('3D Scatter Plot of the Entire Test Dataset')

legend = ax.legend()
ax.add_artist(legend)

plt.show()

"""## Same Data with IsolationForest"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
from sklearn.metrics import classification_report, confusion_matrix

# Parameters
num_unique_train_values = 1000  # Number of unique values for the new column in the training dataset
num_unique_test_values = 20  # Number of unique values for the new column in the testing dataset
num_x_per_value = 9  # Number of different x values per unique value (fixed x values given)
m_min, m_max = 10, 15  # Range for random m values
a_min, a_max = 5, 10  # Range for random a values (ensuring a decreasing trend)

# Fixed x values
x_values = np.array([0.65, 0.70, 0.75, 0.80, 0.85, 0.90,0.95, 1.0, 1.05])

# Function to generate y values for each group ensuring a decreasing trend
def generate_y_values(m, a, x_values):
    return m * np.exp(-a * x_values)

# Generate the training dataset
np.random.seed(42)  # For reproducibility
train_unique_values = np.arange(num_unique_train_values)  # Unique values for the new column in the training dataset
train_m_values = np.random.uniform(m_min, m_max, num_unique_train_values)  # Random m values for each unique value
train_a_values = np.random.uniform(a_min, a_max, num_unique_train_values)  # Random a values for each unique value

# Calculate y values ensuring a decreasing trend for training data
train_y_values = np.concatenate([generate_y_values(train_m_values[i], train_a_values[i], x_values)
                                 for i in range(num_unique_train_values)])

# Prepare the training data
train_new_column = np.repeat(train_unique_values, num_x_per_value)
X_train = np.column_stack((train_new_column, np.tile(x_values, num_unique_train_values), train_y_values))

# Standardize the training data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

# Generate the testing dataset
test_unique_values = np.arange(num_unique_test_values)  # Unique values for the new column in the testing dataset
test_m_values = np.random.uniform(m_min, m_max, num_unique_test_values)  # Random m values for each unique value
test_a_values = np.random.uniform(a_min, a_max, num_unique_test_values)  # Random a values for each unique value

# Calculate y values ensuring a decreasing trend for testing data
test_y_values = np.concatenate([generate_y_values(test_m_values[i], test_a_values[i], x_values)
                                for i in range(num_unique_test_values)])

# Create labels for the test data: 0 for normal data, 1 for anomalies
test_labels = np.zeros(num_unique_test_values * num_x_per_value)

# Inject one anomaly in each group for testing data
for i in range(num_unique_test_values):
    start_idx = i * num_x_per_value
    end_idx = start_idx + num_x_per_value
    anomaly_index = np.random.choice(range(start_idx, end_idx))
    test_y_values[anomaly_index] *= np.random.uniform(0.01, 0.9)  # Inject anomalies by scaling down
    test_labels[anomaly_index] = 1

# Prepare the testing data
test_new_column = np.repeat(test_unique_values, num_x_per_value)
X_test = np.column_stack((test_new_column, np.tile(x_values, num_unique_test_values), test_y_values))
y_test = test_labels

# Standardize the testing data
X_test_scaled = scaler.transform(X_test)

# Train Isolation Forest on the training data
iso_forest = IsolationForest(contamination=0.02, random_state=42)
iso_forest.fit(X_train_scaled)

# Predict anomalies on the test data
y_pred = iso_forest.predict(X_test_scaled)
y_pred = np.where(y_pred == -1, 1, 0)  # Convert predictions to 0 for normal and 1 for anomalies

# Evaluate the model
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

# Plot the data and predictions for each group level
fig, axes = plt.subplots(num_unique_test_values, 2, figsize=(10, num_unique_test_values * 3))

for i in range(num_unique_test_values):
    start_idx = i * num_x_per_value
    end_idx = start_idx + num_x_per_value
    x_group = x_values
    y_group = test_y_values[start_idx:end_idx]
    labels_group = test_labels[start_idx:end_idx]
    y_pred_group = y_pred[start_idx:end_idx]

    # Original data with anomalies for each group
    scatter1 = axes[i, 0].scatter(x_group, y_group, c=labels_group, cmap='coolwarm', zorder=1)
    axes[i, 0].set_title(f'Group {i}: Original Data with Anomalies')
    axes[i, 0].set_xlabel('x')
    axes[i, 0].set_ylabel('y')
    if i == 0:
        legend1 = axes[i, 0].legend(*scatter1.legend_elements(), title="Labels")
        axes[i, 0].add_artist(legend1)
        axes[i, 0].legend(loc='upper right')

    # Isolation Forest predictions for each group
    scatter2 = axes[i, 1].scatter(x_group, y_group, c=y_pred_group, cmap='coolwarm', zorder=1)
    axes[i, 1].set_title(f'Group {i}: Isolation Forest Predictions')
    axes[i, 1].set_xlabel('x')
    axes[i, 1].set_ylabel('y')
    if i == 0:
        legend2 = axes[i, 1].legend(*scatter2.legend_elements(), title="Predictions")
        axes[i, 1].add_artist(legend2)
        axes[i, 1].legend(loc='upper right')

plt.tight_layout()
plt.show()

# 3D plot of the entire test dataset
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')
scatter = ax.scatter(X_test[:, 0], X_test[:, 1], X_test[:, 2], c=test_labels, cmap='coolwarm', label='Original Labels', zorder=1)
correct_anomalies = (y_test == 1) & (y_pred == 1)
scatter_correct = ax.scatter(X_test[correct_anomalies, 0], X_test[correct_anomalies, 1], X_test[correct_anomalies, 2],
                             c='green', marker='^', label='Correct Anomalies', zorder=2)

ax.set_xlabel('Group')
ax.set_ylabel('X')
ax.set_zlabel('Y')
ax.set_title('3D Scatter Plot of the Entire Test Dataset')

legend = ax.legend()
ax.add_artist(legend)

plt.show()

"""## Polynomial Regressor"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures

# Parameters
num_unique_train_values = 1000  # Number of unique values for the new column in the training dataset
num_unique_test_values = 20  # Number of unique values for the new column in the testing dataset
num_x_per_value = 9  # Number of different x values per unique value (fixed x values given)
m_min, m_max = 10, 15  # Range for random m values
a_min, a_max = 5, 10  # Range for random a values (ensuring a decreasing trend)

# Fixed x values
x_values = np.array([0.65, 0.70, 0.75, 0.80, 0.85,0.90, 0.95, 1.0, 1.05])

# Function to generate y values for each group ensuring a decreasing trend
def generate_y_values(m, a, x_values):
    return m * np.exp(-a * x_values)

# Generate the training dataset
np.random.seed(42)  # For reproducibility
train_unique_values = np.arange(num_unique_train_values)  # Unique values for the new column in the training dataset
train_m_values = np.random.uniform(m_min, m_max, num_unique_train_values)  # Random m values for each unique value
train_a_values = np.random.uniform(a_min, a_max, num_unique_train_values)  # Random a values for each unique value

# Calculate y values ensuring a decreasing trend for training data
train_y_values = np.concatenate([generate_y_values(train_m_values[i], train_a_values[i], x_values)
                                 for i in range(num_unique_train_values)])

# Prepare the training data
train_new_column = np.repeat(train_unique_values, num_x_per_value)
X_train = np.column_stack((train_new_column, np.tile(x_values, num_unique_train_values), train_y_values))

# Generate the testing dataset
test_unique_values = np.arange(num_unique_test_values)  # Unique values for the new column in the testing dataset
test_m_values = np.random.uniform(m_min, m_max, num_unique_test_values)  # Random m values for each unique value
test_a_values = np.random.uniform(a_min, a_max, num_unique_test_values)  # Random a values for each unique value

# Calculate y values ensuring a decreasing trend for testing data
test_y_values = np.concatenate([generate_y_values(test_m_values[i], test_a_values[i], x_values)
                                for i in range(num_unique_test_values)])

# Create labels for the test data: 0 for normal data, 1 for anomalies
test_labels = np.zeros(num_unique_test_values * num_x_per_value)

# Inject one anomaly in each group for testing data
for i in range(num_unique_test_values):
    start_idx = i * num_x_per_value
    end_idx = start_idx + num_x_per_value
    anomaly_index = np.random.choice(range(start_idx, end_idx))
    test_y_values[anomaly_index] *= np.random.uniform(0.01, 0.9)  # Inject anomalies by scaling down
    test_labels[anomaly_index] = 1

# Prepare the testing data
test_new_column = np.repeat(test_unique_values, num_x_per_value)
X_test = np.column_stack((test_new_column, np.tile(x_values, num_unique_test_values), test_y_values))
y_test = test_labels

# Polynomial regression for each group
degree = 2  # Degree of the polynomial
poly = PolynomialFeatures(degree)
model = LinearRegression()

# Fit polynomial regression to each group in the training data and detect anomalies
anomalies = np.zeros_like(y_test)
for group in np.unique(test_new_column):
    # Training
    train_group_indices = np.where(train_new_column == group)[0]
    X_train_group = np.tile(x_values, (len(train_group_indices) // num_x_per_value, 1)).flatten().reshape(-1, 1)
    y_train_group = train_y_values[train_group_indices]

    X_train_poly = poly.fit_transform(X_train_group)
    model.fit(X_train_poly, y_train_group)

    # Testing
    test_group_indices = np.where(test_new_column == group)[0]
    X_test_group = np.tile(x_values, (len(test_group_indices) // num_x_per_value, 1)).flatten().reshape(-1, 1)
    y_test_group = test_y_values[test_group_indices]

    X_test_poly = poly.transform(X_test_group)
    y_pred = model.predict(X_test_poly)

    # Detect anomalies based on residuals
    residuals = np.abs(y_test_group - y_pred)
    threshold = 2 * np.std(residuals)  # Define a threshold for anomaly detection
    anomalies[test_group_indices] = residuals > threshold

# Evaluate the model
print(classification_report(y_test, anomalies))
print(confusion_matrix(y_test, anomalies))

# Plot the data and predictions for each group level
fig, axes = plt.subplots(num_unique_test_values, 2, figsize=(10, num_unique_test_values * 3))

for i in range(num_unique_test_values):
    start_idx = i * num_x_per_value
    end_idx = start_idx + num_x_per_value
    x_group = x_values
    y_group = test_y_values[start_idx:end_idx]
    labels_group = test_labels[start_idx:end_idx]
    y_pred_group = anomalies[start_idx:end_idx]

    # Original data with anomalies for each group
    scatter1 = axes[i, 0].scatter(x_group, y_group, c=labels_group, cmap='coolwarm', zorder=1)
    axes[i, 0].set_title(f'Group {i}: Original Data with Anomalies')
    axes[i, 0].set_xlabel('x')
    axes[i, 0].set_ylabel('y')
    if i == 0:
        legend1 = axes[i, 0].legend(*scatter1.legend_elements(), title="Labels")
        axes[i, 0].add_artist(legend1)
        axes[i, 0].legend(loc='upper right')

    # Polynomial regression predictions for each group
    scatter2 = axes[i, 1].scatter(x_group, y_group, c=y_pred_group, cmap='coolwarm', zorder=1)
    axes[i, 1].set_title(f'Group {i}: Polynomial Regression Predictions')
    axes[i, 1].set_xlabel('x')
    axes[i, 1].set_ylabel('y')
    if i == 0:
        legend2 = axes[i, 1].legend(*scatter2.legend_elements(), title="Predictions")
        axes[i, 1].add_artist(legend2)
        axes[i, 1].legend(loc='upper right')

plt.tight_layout()
plt.show()

# 3D plot of the entire test dataset
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')
scatter = ax.scatter(X_test[:, 0], X_test[:, 1], X_test[:, 2], c=test_labels, cmap='coolwarm', label='Original Labels', zorder=1)
correct_anomalies = (y_test == 1) & (anomalies == 1)
scatter_correct = ax.scatter(X_test[correct_anomalies, 0], X_test[correct_anomalies, 1], X_test[correct_anomalies, 2],
                             c='green', marker='^', label='Correct Anomalies', zorder=2)

ax.set_xlabel('Group')
ax.set_ylabel('X')
ax.set_zlabel('Y')
ax.set_title('3D Scatter Plot of the Entire Test Dataset')

legend = ax.legend()
ax.add_artist(legend)

plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures

# Function to generate results based on the formula m * e^(-a * x)
def generate_results(voltage, temp, a, m):
    results_voltage = m * np.exp(-a * voltage)
    results_temp = m * np.exp(-a * temp)
    return results_voltage, results_temp

# Generate dataset
np.random.seed(42)
meas_values = ['A', 'B']
instance_values = ['X', 'Y']
voltage_values = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9])
temp_values = np.array([10, 20, 30, 40, 50, 60])
num_measurements = len(meas_values) * len(instance_values) * len(voltage_values) * len(temp_values)

data_list = []

a_range = (0.1, 1.0)
m_range = (10, 100)

for meas in meas_values:
    for instance in instance_values:
        a = np.random.uniform(*a_range)
        m = np.random.uniform(*m_range)
        for temp in temp_values:
            results_voltage, _ = generate_results(voltage_values, temp, a, m)
            for voltage, result in zip(voltage_values, results_voltage):
                data_list.append({
                    'meas': meas,
                    'instance': instance,
                    'voltage': voltage,
                    'temp': temp,
                    'results': result
                })

data = pd.DataFrame(data_list)

# Group data by 'meas' and 'instance'
grouped = data.groupby(['meas', 'instance'])

# Function to detect anomalies in a subset of data
def detect_anomalies(subset):
    degree = 2  # Degree of the polynomial
    anomalies = []

    # Check voltage vs results (for each temp)
    for temp in subset['temp'].unique():
        temp_subset = subset[subset['temp'] == temp]
        poly = PolynomialFeatures(degree)
        X_poly = poly.fit_transform(temp_subset[['voltage']])
        model = LinearRegression().fit(X_poly, temp_subset['results'])
        y_pred = model.predict(X_poly)
        residuals = np.abs(temp_subset['results'] - y_pred)
        threshold = 2 * np.std(residuals)
        anomalies.extend(temp_subset[residuals > threshold].index.tolist())

    # Check temp vs results (for each voltage)
    for voltage in subset['voltage'].unique():
        voltage_subset = subset[subset['voltage'] == voltage]
        poly = PolynomialFeatures(degree)
        X_poly = poly.fit_transform(voltage_subset[['temp']])
        model = LinearRegression().fit(X_poly, voltage_subset['results'])
        y_pred = model.predict(X_poly)
        residuals = np.abs(voltage_subset['results'] - y_pred)
        threshold = 2 * np.std(residuals)
        anomalies.extend(voltage_subset[residuals > threshold].index.tolist())

    return anomalies

# Apply anomaly detection to each group
all_anomalies = []
for (meas, instance), subset in grouped:
    anomalies = detect_anomalies(subset)
    all_anomalies.extend(anomalies)

# Mark anomalies in the original data
data['is_anomaly'] = False
data.loc[all_anomalies, 'is_anomaly'] = True

# Plotting function for a specific meas and instance
def plot_results_for_specific_group(data, meas, instance, temp_values):
    group_data = data[(data['meas'] == meas) & (data['instance'] == instance)]

    for temp in temp_values:
        subset = group_data[group_data['temp'] == temp]
        plt.figure(figsize=(10, 6))
        plt.scatter(subset['voltage'], subset['results'], c=subset['is_anomaly'].map({True: 'red', False: 'blue'}))
        plt.xlabel('Voltage')
        plt.ylabel('Results')
        plt.title(f'Voltage vs Results for Temp {temp} (Meas: {meas}, Instance: {instance})')
        plt.show()

# Plot results for a specific meas and instance
plot_results_for_specific_group(data, 'A', 'X', [10, 20])

# Output results
print(data)
print("Number of anomalies detected:", data['is_anomaly'].sum())

"""###This Is new testing with Window Method

"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, confusion_matrix
from mpl_toolkits.mplot3d import Axes3D
from sklearn.decomposition import PCA

# Parameters
num_unique_train_values = 1000  # Number of unique values for the new column in the training dataset
num_unique_test_values = 20  # Number of unique values for the new column in the testing dataset
num_x_per_value = 8  # Number of different x values per unique value (fixed x values given)
m_min, m_max = 10, 15  # Range for random m values
a_min, a_max = 5, 10  # Range for random a values (increased to create more curving nature)

# Fixed x values
x_values = np.array([0.65, 0.70, 0.75, 0.80, 0.85,0.90, 0.95, 1.0, 1.05])

# Generate the training dataset
np.random.seed(42)  # For reproducibility
train_unique_values = np.arange(num_unique_train_values)  # Unique values for the new column in the training dataset
train_m_values = np.random.uniform(m_min, m_max, num_unique_train_values)  # Random m values for each unique value
train_a_values = np.random.uniform(a_min, a_max, num_unique_train_values)  # Random a values for each unique value

# Calculate y values (ensuring a decreasing curve) for training data
train_y_values = np.zeros(num_unique_train_values * num_x_per_value)
for i in range(num_unique_train_values):
    start_idx = i * num_x_per_value
    end_idx = start_idx + num_x_per_value
    train_y_values[start_idx:end_idx] = train_m_values[i] * np.exp(-train_a_values[i] * x_values)

# Prepare the training data
train_new_column = np.repeat(train_unique_values, num_x_per_value)
X_train = np.column_stack((train_new_column, np.tile(x_values, num_unique_train_values), train_y_values))

# Standardize the training data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

pca = PCA(n_components=0.95)  # Retain 95% of variance
X_train_scaled = pca.fit_transform(X_train_scaled)


# Generate the testing dataset
test_unique_values = np.arange(num_unique_test_values)  # Unique values for the new column in the testing dataset
test_m_values = np.random.uniform(m_min, m_max, num_unique_test_values)  # Random m values for each unique value
test_a_values = np.random.uniform(a_min, a_max, num_unique_test_values)  # Random a values for each unique value

# Calculate y values (ensuring a decreasing curve) for testing data
test_y_values = np.zeros(num_unique_test_values * num_x_per_value)
for i in range(num_unique_test_values):
    start_idx = i * num_x_per_value
    end_idx = start_idx + num_x_per_value
    test_y_values[start_idx:end_idx] = test_m_values[i] * np.exp(-test_a_values[i] * x_values)

# Create labels for the test data: 0 for normal data, 1 for anomalies
test_labels = np.zeros(num_unique_test_values * num_x_per_value)

# Inject one anomaly in each group for testing data
for i in range(num_unique_test_values):
    start_idx = i * num_x_per_value
    end_idx = start_idx + num_x_per_value
    anomaly_index = np.random.choice(range(start_idx, end_idx))
    test_y_values[anomaly_index] *= np.random.uniform(0.01, 0.9)  # Inject anomalies by scaling down
    test_labels[anomaly_index] = 1

# Prepare the testing data
test_new_column = np.repeat(test_unique_values, num_x_per_value)
X_test = np.column_stack((test_new_column, np.tile(x_values, num_unique_test_values), test_y_values))
y_test = test_labels

# Standardize the testing data
X_test_scaled = scaler.transform(X_test)
X_test_scaled = pca.transform(X_test_scaled)

# Build the MLP classifier
mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)

# Train the MLP classifier
mlp.fit(X_train_scaled, np.zeros(X_train_scaled.shape[0]))  # Training with no anomalies

# Predict using the trained model
y_pred = mlp.predict(X_test_scaled)

# Evaluate the model
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

# Plot the data and predictions for each group level
fig, axes = plt.subplots(num_unique_test_values, 2, figsize=(10, num_unique_test_values * 3))

for i in range(num_unique_test_values):
    start_idx = i * num_x_per_value
    end_idx = start_idx + num_x_per_value
    x_group = x_values
    y_group = test_y_values[start_idx:end_idx]
    labels_group = test_labels[start_idx:end_idx]
    y_pred_group = y_pred[start_idx:end_idx]

    # Original data with anomalies for each group
    scatter1 = axes[i, 0].scatter(x_group, y_group, c=labels_group, cmap='coolwarm', zorder=1)
    axes[i, 0].set_title(f'Group {i}: Original Data with Anomalies')
    axes[i, 0].set_xlabel('x')
    axes[i, 0].set_ylabel('y')
    if i == 0:
        legend1 = axes[i, 0].legend(*scatter1.legend_elements(), title="Labels")
        axes[i, 0].add_artist(legend1)
        axes[i, 0].legend(loc='upper right')

    # MLP classifier predictions for each group
    scatter2 = axes[i, 1].scatter(x_group, y_group, c=y_pred_group, cmap='coolwarm', zorder=1)
    axes[i, 1].set_title(f'Group {i}: MLP Classifier Predictions')
    axes[i, 1].set_xlabel('x')
    axes[i, 1].set_ylabel('y')
    if i == 0:
        legend2 = axes[i, 1].legend(*scatter2.legend_elements(), title="Predictions")
        axes[i, 1].add_artist(legend2)
        axes[i, 1].legend(loc='upper right')

plt.tight_layout()
plt.show()

# 3D plot of the entire test dataset
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')
scatter = ax.scatter(X_test[:, 0], X_test[:, 1], X_test[:, 2], c=test_labels, cmap='coolwarm', label='Original Labels', zorder=1)
correct_anomalies = (y_test == 1) & (y_pred == 1)
scatter_correct = ax.scatter(X_test[correct_anomalies, 0], X_test[correct_anomalies, 1], X_test[correct_anomalies, 2],
                             c='green', marker='^', label='Correct Anomalies', zorder=2)

ax.set_xlabel('Group')
ax.set_ylabel

import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split

# Parameters
num_groups = 100  # Number of groups in the dataset
num_x_per_group = 9  # Number of different x values per group
anomaly_percentage = 0.1  # Percentage of groups with anomalies
m_min, m_max = 10, 15  # Range for random m values
a_min, a_max = 5, 10  # Range for random a values

# Fixed x values
x_values = np.array([0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 1.0, 1.05])

# Generate the dataset
np.random.seed(42)  # For reproducibility
group_values = np.arange(num_groups)  # Unique values for the new column in the dataset
m_values = np.random.uniform(m_min, m_max, num_groups)  # Random m values for each group
a_values = np.random.uniform(a_min, a_max, num_groups)  # Random a values for each group

# Calculate y values (ensuring a decreasing curve for now later on we will add increasing curve too) for each group
y_values = np.zeros(num_groups * num_x_per_group)
for i in range(num_groups):
    start_idx = i * num_x_per_group
    end_idx = start_idx + num_x_per_group
    y_values[start_idx:end_idx] = m_values[i] * np.exp(-a_values[i] * x_values)

# Inject anomalies in 10% of the groups
num_anomalous_groups = int(num_groups * anomaly_percentage)
anomalous_group_indices = np.random.choice(num_groups, num_anomalous_groups, replace=False)
is_anomaly = np.zeros(num_groups * num_x_per_group)

for idx in anomalous_group_indices:
    start_idx = idx * num_x_per_group
    end_idx = start_idx + num_x_per_group
    anomaly_index = np.random.choice(range(start_idx, end_idx))
    y_values[anomaly_index] *= np.random.uniform(0.01, 0.9)  # Inject anomaly by scaling down
    is_anomaly[anomaly_index] = 1


print(f"x values is {x_values}")
print(f"y valiues is {y_values}")
# Prepare the dataset
group_column = np.repeat(group_values, num_x_per_group)
X = np.column_stack((group_column, np.tile(x_values, num_groups), y_values))
y = is_anomaly

print(f"without x scaled {X}")
print(f"without y scaled {y}")
# Standardize the dataset
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print(f"befor x reshaped : {X_scaled}")
print(f"before y reshaped: {y}")
# Reshape the data to pass all 9 rows for each group at a time
X_grouped = X_scaled.reshape(num_groups, num_x_per_group, -1)
y_grouped = y.reshape(num_groups, num_x_per_group)

print(f"after x reshaped : {X_grouped}")
print(f"after y reshaped: {y_grouped}")
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_grouped, y_grouped, test_size=1, random_state=42)

# Flatten the data for the MLP input
X_train_flattened = X_train.reshape(-1, X_train.shape[-1])
y_train_flattened = y_train.reshape(-1)
X_test_flattened = X_test.reshape(-1, X_test.shape[-1])
y_test_flattened = y_test.reshape(-1)

# Build the MLP classifier with 3 hidden layers
mlp = MLPClassifier(hidden_layer_sizes=(100, 100, 100), max_iter=1000, random_state=42)

# Train the MLP classifier
mlp.fit(X_train_flattened, y_train_flattened)

# Predict using the trained model
y_pred = mlp.predict(X_test_flattened)

# Evaluate the model
print(classification_report(y_test_flattened, y_pred))
print(confusion_matrix(y_test_flattened, y_pred))

# Plot the original data and predictions for a sample of groups
sample_groups = 10  # Number of groups to visualize
fig, axes = plt.subplots(sample_groups, 2, figsize=(10, sample_groups * 3))

# Extract original X and y values for the test set
X_test_original = scaler.inverse_transform(X_test_flattened)
group_column_test = X_test_original[:, 0].astype(int)
x_values_test = X_test_original[:, 1]
y_values_test = X_test_original[:, 2]

for i in range(sample_groups):
    group_idx = np.where(group_column_test == i)[0]
    if len(group_idx) == 0:
        continue
    x_group = x_values_test[group_idx]
    y_group = y_values_test[group_idx]
    labels_group = y_test_flattened[group_idx]
    y_pred_group = y_pred[group_idx]

    # Original data with anomalies for each group
    scatter1 = axes[i, 0].scatter(x_group, y_group, c=labels_group, cmap='coolwarm', zorder=1)
    axes[i, 0].set_title(f'Group {i}: Original Data with Anomalies')
    axes[i, 0].set_xlabel('x')
    axes[i, 0].set_ylabel('y')
    if i == 0:
        legend1 = axes[i, 0].legend(*scatter1.legend_elements(), title="Labels")
        axes[i, 0].add_artist(legend1)
        axes[i, 0].legend(loc='upper right')

    # MLP classifier predictions for each group
    scatter2 = axes[i, 1].scatter(x_group, y_group, c=y_pred_group, cmap='coolwarm', zorder=1)
    axes[i, 1].set_title(f'Group {i}: MLP Classifier Predictions')
    axes[i, 1].set_xlabel('x')
    axes[i, 1].set_ylabel('y')
    if i == 0:
        legend2 = axes[i, 1].legend(*scatter2.legend_elements(), title="Predictions")
        axes[i, 1].add_artist(legend2)
        axes[i, 1].legend(loc='upper right')

plt.tight_layout()
plt.show()

"""## traing + cv"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Parameters
num_groups = 1000  # Number of groups in the dataset
num_x_per_group = 9  # Number of different x values per group
initial_anomaly_percentage = 0.1  # Percentage of groups with initial anomalies
new_anomaly_percentage = 0.3  # Percentage of new anomalies
m_min, m_max = 10, 15  # Range for random m values
a_min, a_max = 5, 10  # Range for random a values

# Fixed x values
x_values = np.array([0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 1.0, 1.05])

# Generate the dataset for testing
np.random.seed(42)  # For reproducibility
group_values = np.arange(num_groups)  # Unique values for the new column in the dataset
m_values = np.random.uniform(m_min, m_max, num_groups)  # Random m values for each group
a_values = np.random.uniform(a_min, a_max, num_groups)  # Random a values for each group

# Calculate y values (ensuring a decreasing curve) for each group
y_values = np.zeros(num_groups * num_x_per_group)
for i in range(num_groups):
    start_idx = i * num_x_per_group
    end_idx = start_idx + num_x_per_group
    y_values[start_idx:end_idx] = m_values[i] * np.exp(-a_values[i] * x_values)

# Inject anomalies in 10% of the groups
num_anomalous_groups = int(num_groups * initial_anomaly_percentage)
anomalous_group_indices = np.random.choice(num_groups, num_anomalous_groups, replace=False)
is_anomaly = np.zeros(num_groups * num_x_per_group)

print("Initial Anomalies:")
for idx in anomalous_group_indices:
    start_idx = idx * num_x_per_group
    end_idx = start_idx + num_x_per_group
    anomaly_index = np.random.choice(range(start_idx, end_idx))
    y_values[anomaly_index] *= np.random.uniform(1.01, 1.9)  # Inject anomaly by scaling up
    is_anomaly[anomaly_index] = 1
    print(f"Group {idx} at index {anomaly_index - start_idx}")

# Prepare the dataset
group_column = np.repeat(group_values, num_x_per_group)
X = np.column_stack((group_column, np.tile(x_values, num_groups), y_values))
y = is_anomaly

# Standardize the dataset
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Reshape the data to pass all 9 rows for each group at a time
X_grouped = X_scaled.reshape(num_groups, num_x_per_group, -1)
y_grouped = y.reshape(num_groups, num_x_per_group)

# Flatten the data for the MLP input
X_flattened = X_grouped.reshape(-1, X_grouped.shape[-1])
y_flattened = y_grouped.reshape(-1)

# Build and train the MLP classifier with 3 hidden layers
mlp = MLPClassifier(hidden_layer_sizes=(100, 100, 100), max_iter=1000, random_state=42)
mlp.fit(X_flattened, y_flattened)

# Predict using the trained model
y_pred = mlp.predict(X_flattened)

# Evaluate the model
print("\nInitial Dataset Evaluation:")
print(classification_report(y_flattened, y_pred))
print(confusion_matrix(y_flattened, y_pred))

# Generate new samples with 30% anomalies
num_anomalous_groups_new = int(num_groups * new_anomaly_percentage)
anomalous_group_indices_new = np.random.choice(num_groups, num_anomalous_groups_new, replace=False)
new_y_values = np.zeros(num_groups * num_x_per_group)
new_is_anomaly = np.zeros(num_groups * num_x_per_group)

for i in range(num_groups):
    start_idx = i * num_x_per_group
    end_idx = start_idx + num_x_per_group
    new_y_values[start_idx:end_idx] = m_values[i] * np.exp(-a_values[i] * x_values)

print("\nNew Anomalies:")
for idx in anomalous_group_indices_new:
    start_idx = idx * num_x_per_group
    end_idx = start_idx + num_x_per_group
    anomaly_index = np.random.choice(range(start_idx, end_idx))
    new_y_values[anomaly_index] *= np.random.uniform(1.01, 1.9)  # Inject anomaly by scaling up
    new_is_anomaly[anomaly_index] = 1
    print(f"Group {idx} at index {anomaly_index - start_idx}")

# Prepare the new dataset
new_X = np.column_stack((group_column, np.tile(x_values, num_groups), new_y_values))
new_y = new_is_anomaly

# Standardize the new dataset
new_X_scaled = scaler.transform(new_X)

# Reshape the new data to pass all 9 rows for each group at a time
new_X_grouped = new_X_scaled.reshape(num_groups, num_x_per_group, -1)
new_y_grouped = new_y.reshape(num_groups, num_x_per_group)

# Flatten the new data for the MLP input
new_X_flattened = new_X_grouped.reshape(-1, new_X_grouped.shape[-1])
new_y_flattened = new_y_grouped.reshape(-1)

# Predict using the pre-trained model on the new dataset
new_y_pred = mlp.predict(new_X_flattened)

# Evaluate the model on the new dataset
print("\nNew Dataset Evaluation:")
print(classification_report(new_y_flattened, new_y_pred))
print(confusion_matrix(new_y_flattened, new_y_pred))

# Plot the original data and predictions for a sample of groups in the new dataset
sample_groups = 10  # Number of groups to visualize
fig, axes = plt.subplots(sample_groups, 2, figsize=(10, sample_groups * 3))

# Extract original X and y values for the new test set
new_X_original = scaler.inverse_transform(new_X_flattened)
group_column_new = new_X_original[:, 0].astype(int)
x_values_new = new_X_original[:, 1]
y_values_new = new_X_original[:, 2]

for i in range(sample_groups):
    group_idx = np.where(group_column_new == i)[0]
    if len(group_idx) == 0:
        continue
    x_group = x_values_new[group_idx]
    y_group = y_values_new[group_idx]
    labels_group = new_y_flattened[group_idx]
    y_pred_group = new_y_pred[group_idx]

    # Original data with anomalies for each group
    scatter1 = axes[i, 0].scatter(x_group, y_group, c=labels_group, cmap='coolwarm', zorder=1)
    axes[i, 0].set_title(f'Group {i}: Original Data with Anomalies')
    axes[i, 0].set_xlabel('x')
    axes[i, 0].set_ylabel('y')
    if i == 0:
        legend1 = axes[i, 0].legend(*scatter1.legend_elements(), title="Labels")
        axes[i, 0].add_artist(legend1)
        axes[i, 0].legend(loc='upper right')

    # MLP classifier predictions for each group
    scatter2 = axes[i, 1].scatter(x_group, y_group, c=y_pred_group, cmap='coolwarm', zorder=1)
    axes[i, 1].set_title(f'Group {i}: MLP Classifier Predictions')
    axes[i, 1].set_xlabel('x')
    axes[i, 1].set_ylabel('y')
    if i == 0:
        legend2 = axes[i, 1].legend(*scatter2.legend_elements(), title="Predictions")
        axes[i, 1].add_artist(legend2)
        axes[i, 1].legend(loc='upper right')

plt.tight_layout()
plt.show()