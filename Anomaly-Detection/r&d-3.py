# -*- coding: utf-8 -*-
"""R&D.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ywXWDBKzhfxwy4snUADgZ2CInX1HswC8
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier

# Assuming 'scaler' and 'mlp' are already defined and trained

def process_csv_and_predict(csv1, csv2, output_csv):
    # Load the CSV files
    df1 = pd.read_csv(csv1)
    df2 = pd.read_csv(csv2)

    # Combine the dataframes
    df = pd.concat([df1, df2])

    # Keep only the necessary columns
    df = df[['INSTANCE', 'temperature', 'voltage', 'value']]

    # Convert INSTANCE and temperature to numeric
    df['INSTANCE'] = df['INSTANCE'].astype('category').cat.codes  # Convert to category codes (int)
    df['temperature'] = df['temperature'].astype(float)  # Ensure temperature is float

    # Create a group column based on "INSTANCE" and "temperature"
    df['group'] = df['INSTANCE'].astype(str) + '_' + df['temperature'].astype(str)

    # Filter out groups with fewer than 9 x values
    group_counts = df['group'].value_counts()
    valid_groups = group_counts[group_counts >= 9].index
    df = df[df['group'].isin(valid_groups)]

    # Prepare the data for prediction
    unique_groups = df['group'].unique()
    X = []

    for group in unique_groups:
        group_data = df[df['group'] == group]
        if len(group_data) == 9:  # Ensure there are exactly 9 rows for prediction
            X.append(group_data[['INSTANCE', 'temperature', 'voltage', 'value']].values)

    # Convert to numpy array
    X = np.array(X).reshape(-1, 4)  # Reshape if necessary (4 features: INSTANCE, temperature, voltage, value)

    # Standardize the new dataset
    X_scaled = scaler.transform(X[:, 1:])  # Use only the numeric features (temperature, voltage, value)

    # Predict anomalies using the pre-trained model
    y_pred = mlp.predict(X_scaled)

    # Add the predictions to the original dataframe
    df['isAnomaly'] = np.nan
    for i, group in enumerate(unique_groups):
        group_indices = df['group'] == group
        if len(df[group_indices]) == 9:  # Check if group has 9 entries
            df.loc[group_indices, 'isAnomaly'] = y_pred[i]

    # Save the results to a new CSV file
    df.to_csv(output_csv, index=False)
    print(f'Results saved to {output_csv}')

# Example usage
csv1 = 'your_first_input_file.csv'
csv2 = 'your_second_input_file.csv'
output_csv = 'predicted_anomalies.csv'
process_csv_and_predict(csv1, csv2, output_csv)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix

# Parameters
num_groups = 1000  # Number of groups in the dataset
num_x_per_group = 9  # Number of different x values per group
initial_anomaly_percentage = 0.4  # Percentage of groups with initial anomalies
new_anomaly_percentage = 0.3  # Percentage of new anomalies
m_min, m_max = 10, 15  # Range for random m values
a_min, a_max = 5, 10  # Range for random a values

# Fixed x values
x_values = np.array([0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95, 1.0, 1.05])

# Generate the dataset for testing
np.random.seed(42)  # For reproducibility
group_values = np.arange(num_groups)  # Unique values for the new column in the dataset
m_values = np.random.uniform(m_min, m_max, num_groups)  # Random m values for each group
a_values = np.random.uniform(a_min, a_max, num_groups)  # Random a values for each group

# Calculate y values (ensuring a decreasing curve) for each group
y_values = np.zeros(num_groups * num_x_per_group)
for i in range(num_groups):
    start_idx = i * num_x_per_group
    end_idx = start_idx + num_x_per_group
    y_values[start_idx:end_idx] = m_values[i] * np.exp(-a_values[i] * x_values)

# Inject anomalies in 10% of the groups
num_anomalous_groups = int(num_groups * initial_anomaly_percentage)
anomalous_group_indices = np.random.choice(num_groups, num_anomalous_groups, replace=False)
is_anomaly = np.zeros(num_groups * num_x_per_group)

print("Initial Anomalies:")
for idx in anomalous_group_indices:
    start_idx = idx * num_x_per_group
    end_idx = start_idx + num_x_per_group
    anomaly_index = np.random.choice(range(start_idx, end_idx))
    y_values[anomaly_index] *= np.random.uniform(0.01, 0.9)  # Inject anomaly by scaling down
    is_anomaly[anomaly_index] = 1
    #print(f"Group {idx} at index {anomaly_index - start_idx}")

# Prepare the dataset
group_column = np.repeat(group_values, num_x_per_group)
X = np.column_stack((group_column, np.tile(x_values, num_groups), y_values))
y = is_anomaly

# Standardize the dataset
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X[:, 1:])  # Remove group column

# Reshape the data to pass all 9 rows for each group at a time
X_grouped = X_scaled.reshape(num_groups, num_x_per_group, -1)
y_grouped = y.reshape(num_groups, num_x_per_group)

print(f"X_grouped data shape is {X_grouped.shape}")
print(f"Y_grouped data shape is {y_grouped.shape}")

# Convert data to PyTorch tensors
X_tensor = torch.tensor(X_grouped, dtype=torch.float32)
y_tensor = torch.tensor(y_grouped, dtype=torch.float32)

# Define the custom MLP model
class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.layer1 = nn.Linear(num_x_per_group * 2, 100)  # input: (num_x_per_group, 2) flattened
        self.layer2 = nn.Linear(100, 100)
        self.layer3 = nn.Linear(100, 100)
        self.output = nn.Linear(100, num_x_per_group)

    def forward(self, x):
        x = x.view(x.size(0), -1)  # Flatten the input
        x = torch.relu(self.layer1(x))
        x = torch.relu(self.layer2(x))
        x = torch.relu(self.layer3(x))
        x = torch.sigmoid(self.output(x))
        return x

# Initialize the model, loss function, and optimizer
model = MLP()
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Train the model
num_epochs = 1000
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    output = model(X_tensor)
    loss = criterion(output, y_tensor)
    loss.backward()
    optimizer.step()
    if (epoch + 1) % 100 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# Predict using the trained model
model.eval()
with torch.no_grad():
    y_pred_tensor = model(X_tensor)
    y_pred = y_pred_tensor.numpy().round()

# Evaluate the model
print("\nInitial Dataset Evaluation:")
print(classification_report(y_grouped.flatten(), y_pred.flatten()))
print(confusion_matrix(y_grouped.flatten(), y_pred.flatten()))

# Generate new samples with 30% anomalies
num_anomalous_groups_new = int(num_groups * new_anomaly_percentage)
anomalous_group_indices_new = np.random.choice(num_groups, num_anomalous_groups_new, replace=False)
new_y_values = np.zeros(num_groups * num_x_per_group)
new_is_anomaly = np.zeros(num_groups * num_x_per_group)

for i in range(num_groups):
    start_idx = i * num_x_per_group
    end_idx = start_idx + num_x_per_group
    new_y_values[start_idx:end_idx] = m_values[i] * np.exp(-a_values[i] * x_values)

print("\nNew Anomalies:")
for idx in anomalous_group_indices_new:
    start_idx = idx * num_x_per_group
    end_idx = start_idx + num_x_per_group
    anomaly_index = np.random.choice(range(start_idx, end_idx))
    new_y_values[anomaly_index] *= np.random.uniform(0.01, 0.9)  # Inject anomaly by scaling down
    new_is_anomaly[anomaly_index] = 1
    #print(f"Group {idx} at index {anomaly_index - start_idx}")

# Prepare the new dataset
new_X = np.column_stack((group_column, np.tile(x_values, num_groups), new_y_values))
new_y = new_is_anomaly

# Standardize the new dataset
new_X_scaled = scaler.transform(new_X[:, 1:])  # Remove group column

# Reshape the new data to pass all 9 rows for each group at a time
new_X_grouped = new_X_scaled.reshape(num_groups, num_x_per_group, -1)
new_y_grouped = new_y.reshape(num_groups, num_x_per_group)

# Convert new data to PyTorch tensors
new_X_tensor = torch.tensor(new_X_grouped, dtype=torch.float32)
new_y_tensor = torch.tensor(new_y_grouped, dtype=torch.float32)

# Predict using the pre-trained model on the new dataset
model.eval()
with torch.no_grad():
    new_y_pred_tensor = model(new_X_tensor)
    new_y_pred = new_y_pred_tensor.numpy().round()

# Evaluate the model on the new dataset
print("\nNew Dataset Evaluation:")
print(classification_report(new_y_grouped.flatten(), new_y_pred.flatten()))
print(confusion_matrix(new_y_grouped.flatten(), new_y_pred.flatten()))

# Plot the original data and predictions for the groups with anomalies in the new dataset
sample_groups = 10  # Number of groups with anomalies to visualize
anomalous_groups_to_plot = anomalous_group_indices_new[:sample_groups]
fig, axes = plt.subplots(len(anomalous_groups_to_plot), 2, figsize=(10, len(anomalous_groups_to_plot) * 3))

# Extract original X and y values for the new test set
new_X_original = scaler.inverse_transform(new_X_grouped.reshape(-1, new_X_grouped.shape[-1]))
group_column_new = np.repeat(group_values, num_x_per_group)
x_values_new = new_X_original[:, 0]
y_values_new = new_X_original[:, 1]

for i, group_idx in enumerate(anomalous_groups_to_plot):
    idx = np.where(group_column_new == group_idx)[0]
    x_group = x_values_new[idx]
    y_group = y_values_new[idx]
    labels_group = new_y_grouped.flatten()[idx]
    y_pred_group = new_y_pred.flatten()[idx]

    # Original data with anomalies for each group
    scatter1 = axes[i, 0].scatter(x_group, y_group, c=labels_group, cmap='coolwarm', zorder=1)
    axes[i, 0].set_title(f'Group {group_idx}: Original Data with Anomalies')
    axes[i, 0].set_xlabel('x')
    axes[i, 0].set_ylabel('y')
    if i == 0:
        legend1 = axes[i, 0].legend(*scatter1.legend_elements(), title="Labels")
        axes[i, 0].add_artist(legend1)
        axes[i, 0].legend(loc='upper right')

    # MLP classifier predictions for each group
    scatter2 = axes[i, 1].scatter(x_group, y_group, c=y_pred_group, cmap='coolwarm', zorder=1)
    axes[i, 1].set_title(f'Group {group_idx}: MLP Classifier Predictions')
    axes[i, 1].set_xlabel('x')
    axes[i, 1].set_ylabel('y')
    if i == 0:
        legend2 = axes[i, 1].legend(*scatter2.legend_elements(), title="Predictions")
        axes[i, 1].add_artist(legend2)
        axes[i, 1].legend(loc='upper right')

plt.tight_layout()
plt.show()